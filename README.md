# Block Blast Game + Reinforcement Learning AI Agent

Block Blast is a Tetris-inspired puzzle game played on an 8Ã—8 grid. At each turn, the player (or agent) is presented with three randomly generated block shapes and must choose one to place anywhere on the board. Whenever an entire row or column is filled, it clears and awards points; clearing multiple lines in succession activates a combo multiplier for even higher scores.

Under the hood, the game engine guarantees that at least one of the three available shapes can always be placed, ensuring no unwinnable states arise prematurely. A Pygame-based interface lets humans play and test strategies interactively, while a custom OpenAI Gym environment exposes the gameâ€™s state, action space, and reward function for Reinforcement Learning.

This repository implements several RL agentsâ€”including a random baseline, Deep Q-Network (DQN), Proximal Policy Optimization (PPO), and a masked-action PPO variantâ€”to train and evaluate performance. Comprehensive scripts are provided to train agents, log metrics, visualize results, and compare different approaches under consistent conditions.

<p align="center">
  <img src="blockblast_game/Assets/demo.png" alt="BlockBlast Gameplay" width="600"/>
</p>

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Project Structure](#-project-structure)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Usage](#-usage)

  - [Human Play](#human-play)
  - [Training Agents](#training-agents)
  - [Comparing Agents](#comparing-agents)

- [Reinforcement Learning Environment](#-reinforcement-learning-environment)
- [Algorithms](#-algorithms)
- [Results & Analysis](#-results--analysis)
- [Future Work](#-future-work)
- [Credits](#-credits)
- [License](#-license)

---

## ğŸš€ Features

- **Custom Gym Env**: Fine-tuned observation, action space, and rewards.
- **Action Masking**: Prevents invalid moves for PPO & DQN.
- **Multiple Agents**:
  - Random (baseline)
  - PPO (with/without masking)
  - DQN
- **Visualization**: TensorBoard logs, matplotlib plots of performance.
- **Human Interface**: Pygame-based play via keyboard & mouse.

---

## ğŸ“‚ Project Structure

```
BlockBlast-Game-AI-Agent/
â”œâ”€â”€ agent_comparison/      # Simulation & comparison scripts
â”‚   â”œâ”€â”€ simulate_playing.py
â”‚   â””â”€â”€ results/           # CSVs & plots
â”œâ”€â”€ agents/                # RL agents & models
â”‚   â”œâ”€â”€ models/            # Saved checkpoints
â”‚   â”œâ”€â”€ ppo_agent.py
â”‚   â””â”€â”€ dqn_agent.py
â”œâ”€â”€ blockblast_game/       # Game environment & assets
â”‚   â”œâ”€â”€ game_env.py
â”‚   â”œâ”€â”€ game_renderer.py
â”‚   â”œâ”€â”€ game_state.py
â”‚   â””â”€â”€ Assets/            # Sprites & images
â”œâ”€â”€ human_play/            # Humanâ€play wrapper
â”‚   â””â”€â”€ human_play.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

- Python 3.8+
- [Pygame](https://www.pygame.org/news)
- [Gym](https://github.com/openai/gym)
- [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3)
- `pip install -r requirements.txt`

---

## ğŸ”§ Installation

1. **Clone the repo**

   ```bash
   git clone https://github.com/RisticDjordje/BlockBlast-Game-AI-Agent.git
   cd BlockBlast-Game-AI-Agent
   ```

2. **Create & activate a virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate    # macOS/Linux
   venv\Scriptsctivate       # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ’» Usage

**Ensure you are running all commands from the root directory of the project to avoid import errors due to sibling folder structure.**

### Human Play

```bash
python -m human_play.human_play
```

- **E, R, T**: Select piece
- **Mouse hover**: Preview placement
- **Left-click / SPACE**: Place piece
- **ESC**: Restart

---

### Training Agents

#### PPO Agent

Edit `agents/ppo_agent.py` flags:

```python
train_ppo_without_masking = False
train_ppo_with_masking    = False
visualize_ppo_without_masking = False
visualize_ppo_with_masking    = False
continue_training= False
```

Run:

```bash
python -m agents.ppo_agent
```

#### DQN Agent

Edit `agents/dqn_agent.py` flags:

```python
train_dqn_agent    = False
visualize_dqn_agent = False
continue_training   = False 
```

Run:

```bash
python -m agents.dqn_agent
```

---

### Comparing Agents

1. **Simulate games**

   ```bash
   python -m agent_comparison.simulate_playing
   ```

   â†’ CSV in `agent_comparison/results/`

2. **Visualize results**
   ```bash
   python -m agent_comparison.visualize_results
   ```
   â†’ Performance plots in `agent_comparison/results/`

---

## ğŸ— Reinforcement Learning Environment

- **Observation**:
  - 8Ã—8 grid (0/1)
  - 3Ã— 5Ã—5 piece matrices
  - Current score & combo count
- **Action Space**: 3 shapes Ã— 64 positions â†’ 192 discrete actions
- **Reward Design** (example although I have tried many different combinations):
  - Valid placement: +0.2
  - Invalid: â€“2.5 (escalating penalties)
  - Line clear: +1.5 / line
  - Game over: â€“20

---

## ğŸ¤– Algorithms

1. **Random Agent**
2. **PPO** (Clipped Policy Gradient)
3. **DQN** (Value-based with Replay & Target Network)
4. **Masked PPO** (invalidâ€action masking)

See inline docstrings for hyperparameters and architecture details.

---

## ğŸ“Š Results & Analysis

- **DQN and PPO**: both struggle with the large action space of the game and I have not been able to get the agents to learn not to place pieces in invalid positions.
- I have solved this by introducing action masking. Masked PPO avoids invalid moves and gains higher average rewards compared to an agent that plays randomly among the valid moves.

---

## ğŸ”­ Future Work

- Actionâ€masking for DQN
- Hyperparameter sweeps & longer training
- MLOps pipeline: Web UI for drawing board state & serving model (e.g., Flask + React)
- Deploy a public â€œBlockBlast Solverâ€ website

---

## ğŸ™ Credits

- **Game Assets**: [Kefrovâ€™s Blast](https://github.com/Kefrov/Blast/)  
  Everything else, including game logic, agent training, and analysis, is original work.

---

## ğŸ“„ License

This project is licensed under the MIT License.
